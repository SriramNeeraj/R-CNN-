Faster R-CNN is a single-stage model that is trained end-to-end. It uses a novel region proposal network (RPN) for generating region proposals, which save time compared to traditional algorithms like Selective Search. It uses the ROI Pooling layer to extract a fixed-length feature vector from each region proposal.Faster RCNN is an object detection architecture presented by Ross Girshick, Shaoqing Ren, Kaiming He and Jian Sun in 2015, and is one of the famous object detection architectures that uses convolution neural networks like YOLO (You Look Only Once) and SSD ( Single Shot Detector).Region-based Convolutional Neural Network
Object detection consists of two separate tasks that are classification and localization. R-CNN stands for Region-based Convolutional Neural Network. The key concept behind the R-CNN series is region proposals. Region proposals are used to localize objects within an image.The Faster-RCNN method is used for face detection and also for face recognition. Inception V2 architecture is utilized due to has a high accuracy among Convolutional Neural Network architecture. The best learning rate and epoch parameters for the Faster R-CNN model are optimized to improve face recognition on CCTV.A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used.One of the most popular object detection methods is the R-CNN series, developed by Ross Girshick et al in 2014, improved upon with Fast R-CNN and then finally with Faster R-CNN.Intuition of Faster RCNN. Faster RCNN is the modified version of Fast RCNN. The major difference between them is that Fast RCNN uses selective search for generating Regions of Interest, while Faster RCNN uses “Region Proposal Network”, aka RPN.CNN Building Blocks

 Each hidden layer is also made up of a set of neurons, where each neuron is fully connected to all neurons in the previous layer. The last layer of a neural network (i.e., the “output layer”) is also fully connected and represents the final output classifications of the network.The different layers of a CNN. There are four types of layers for a convolutional neural network: the convolutional layer, the pooling layer, the ReLU correction layer and the fully-connected layer.Principles/Conventions to build a CNN architecture

 Following the principle, the number of channels should be low in the beginning such that it detects low-level features which are combined to form many complex shapes(by increasing the number of channels) which help distinguish between classes.After the improvement in architecture of object detection network in R-CNN to Fast R_CNN. The training and detection time of the network decrease considerably, but the network is not fast enough to be used as a real-time system because it takes approximately (2 seconds) to generate output on an input image. The bottleneck of architecture is a selective search algorithm. Therefore K He et al. proposed a new architecture called Faster R-CNN. It does not use selective search instead they propose another region proposal generation algorithm called Region Proposal Network. Let’s discuss the Faster R-CNN architectureFaster R-CNN architecture contains 2 networks: 
 

Region Proposal Network (RPN)
Object Detection Network
Before discussing the Region proposal we need to look into the CNN architecture which is the backbone of this network. This CNN architecture is common between both Region Proposal Network and Object Detection Network. We experimented with ZF (which has 5 shareable Conv layers) or VGG-16 (which has 13 shareable Conv) as the backbone in their architecture. Both backbone network has the network stride of 16 which means an image of dimension 1000 * 600 is reduced to (1000/16 * 600/16) or approximately (~ 62 *37) size feature map before passing into region proposal network. 

Region Proposal Network (RPN): 
This region proposal network takes convolution feature map that is generated by the backbone layer as input and outputs the anchors generated by sliding window convolution applied on the input feature map.For each sliding window, the network generates the maximum number of k- anchor boxes. By the default the value of k=9 (3 scales of (128*128, 256*256 and 512*512) and 3 aspect ratio of (1:1, 1:2 and 2:1)) for each of different sliding position in image. Therefore, for a convolution feature map of W * H, we get N = W* H* k anchor boxes. These region proposals then passed into an intermediate layer of 3*3 convolution and 1 padding and 256 (for ZF) or 512 (for VGG-16 ) output channels. The output generated from this layer is passed into two layers of 1*1 convolution, the classification layer, and the regression layer. the regression layer has 4*N (W * H * (4*k)) output parameters (denoting the coordinates of bounding boxes) and the classification layer has 2*N (W * H * (2*k)) output parameters (denoting the probability of object or not object). Training and Loss Function (RPN) : 
First of all, we remove all the cross-boundary anchors, so, that they do not increase the loss function. For a typical 1000*600 image, there are roughly 20000(~ 60*40*9) anchors. If we remove the cross-boundary anchors then there are roughly 6000 anchors left per image. The paper also uses Non-Maximum Suppression based on their classification and IoU. Here they use a fixed IoU of 0.7. This also reduces the number of anchors to 2000. The advantage of using Non-Maximum suppression that it also doesn’t hurt accuracy as well. RPN can be trained end to end by using backpropagation and stochastic gradient descent. It generates each mini-batch from the anchors of a single image. It does not train loss function on each anchor instead it selects 256 random anchors with positive and negative sample s in the ratio of 1:1. If an image contains <128 positives then it uses more negative samples. For training RPNs, First, we need to assign binary class label (weather the concerned anchor contains an object or background). In the faster R-CNN paper, the author uses two conditions to assign a positive label to an anchor. These are : 
 

those anchors which have the highest Intersection-over-Union (IoU) with a ground-truth box, or
an anchor that has an IoU overlap higher than 0.7 with any ground-truth box.
and negative label to those which has IoU overlap is <0.3 for all ground truth boxes. Those anchors which does not have either positive or negative label does not contribute to training.
